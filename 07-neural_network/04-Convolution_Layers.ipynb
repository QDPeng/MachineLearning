{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.26839399  2.60158466 -1.24131294 -0.45502629 -1.77953067  0.13782581\n",
      "  0.22095009 -1.03822071  0.76556819  0.43275458  1.52816032 -1.94326924\n",
      "  0.05509926  2.11127391  0.61317289 -0.07634732 -1.16257783  0.63697826\n",
      " -0.32768125  1.20707562 -0.98744165  0.84372634 -0.22266482  0.57540839\n",
      "  0.77225308]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# ---------------------------------------------------|\n",
    "# -------------------1D-data-------------------------|\n",
    "# ---------------------------------------------------|\n",
    "\n",
    "# Create graph session \n",
    "sess = tf.Session()\n",
    "# Generate 1D data\n",
    "data_size = 25\n",
    "data_1d = np.random.normal(size=data_size)\n",
    "print(data_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_96:0\", shape=(1, 1, 21, 1), dtype=float32) 5\n",
      "Tensor(\"MaxPool_13:0\", shape=(1, 1, 17, 1), dtype=float32)\n",
      "Tensor(\"Squeeze_53:0\", shape=(17,), dtype=float32)\n",
      "full_connected\n",
      "(17,)\n",
      "Tensor(\"Shape_23:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"stack_17:0\", shape=(2, 1), dtype=int32)\n",
      "Tensor(\"Squeeze_54:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"Squeeze_54:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"random_normal_41:0\", shape=(5,), dtype=float32)\n",
      "Tensor(\"Add_13:0\", shape=(1, 5), dtype=float32)\n",
      "Tensor(\"Squeeze_55:0\", shape=(5,), dtype=float32)\n",
      "Help on function shape in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "shape(input, name=None, out_type=tf.int32)\n",
      "    Returns the shape of a tensor.\n",
      "    \n",
      "    This operation returns a 1-D integer tensor representing the shape of `input`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\n",
      "    shape(t) ==> [2, 2, 3]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor` or `SparseTensor`.\n",
      "      name: A name for the operation (optional).\n",
      "      out_type: (Optional) The specified output type of the operation\n",
      "        (`int32` or `int64`). Defaults to `tf.int32`.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of type `out_type`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Placeholder\n",
    "x_input_1d = tf.placeholder(shape=[data_size],dtype=tf.float32)\n",
    "# Create filter for convolution.\n",
    "conv_filter_1 = tf.Variable(tf.random_normal(shape=[1,5,1,1]))\n",
    "\n",
    "# --------Convolution--------\n",
    "# input shape = [batch, in_height, in_width, in_channels]\n",
    "# filter shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "def conv_layers_1d(input_1d,my_filter):\n",
    "    input_2d = tf.expand_dims(input_1d,0)\n",
    "    input_3d = tf.expand_dims(input_2d,0)\n",
    "    input_4d = tf.expand_dims(input_3d,3)\n",
    "    #input_4d like (1,1,input_1d,1)\n",
    "    convolution_output = tf.nn.conv2d(input_4d,filter=my_filter,strides=[1,1,1,1],padding='VALID')\n",
    "    #print(convolution_output) #Tensor(\"Conv2D_4:0\", shape=(1, 1, 21, 1), dtype=float32)\n",
    "    conv_output_1d = tf.squeeze(convolution_output)\n",
    "#     print(conv_output_1) #Tensor(\"Squeeze:0\", shape=(21,), dtype=float32)\n",
    "    return conv_output_1d\n",
    "# Create convolution layer\n",
    "conv_output_1 = conv_layers_1d(x_input_1d,conv_filter_1)\n",
    "# --------Activation Function--------\n",
    "def activation(input_1d):\n",
    "    return tf.nn.relu(input_1d)\n",
    "\n",
    "# Create activation layer\n",
    "activation_output_1 = activation(conv_output_1)\n",
    "# --------Max Pool--------\n",
    "'''\n",
    "max pooling是CNN当中的最大值池化操作，其实用法和卷积很类似\n",
    "\n",
    "有些地方可以从卷积去参考【TensorFlow】tf.nn.conv2d是怎样实现卷积的？ \n",
    "\n",
    "tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
    "参数是四个，和卷积很类似：\n",
    "\n",
    "第一个参数value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape\n",
    "\n",
    "第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "\n",
    "第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "\n",
    "第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'\n",
    "\n",
    "返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式\n",
    "\n",
    "'''\n",
    "def max_pool(input_1d,width):\n",
    "    # Just like 'conv2d()' above, max_pool() works with 4D arrays.\n",
    "    # [batch_size=1, width=1, height=num_input, channels=1]\n",
    "    input_2d = tf.expand_dims(input_1d,0)\n",
    "    input_3d = tf.expand_dims(input_2d,0)\n",
    "    input_4d = tf.expand_dims(input_3d,3)\n",
    "    print(input_4d,width) #Tensor(\"ExpandDims_19:0\", shape=(1, 1, 21, 1), dtype=float32) 5\n",
    "    pool_output = tf.nn.max_pool(input_4d,ksize=[1,1,width,1],strides=[1,1,1,1],padding='VALID')\n",
    "    print(pool_output)#Tensor(\"MaxPool_2:0\", shape=(1, 1, 17, 1), dtype=float32)\n",
    "    # Get rid of extra dimensions\n",
    "    pool_output_1d = tf.squeeze(pool_output)\n",
    "    print(pool_output_1d)#Tensor(\"Squeeze_9:0\", shape=(17,), dtype=float32)\n",
    "    return pool_output_1d\n",
    "\n",
    "maxpool_output_1 = max_pool(activation_output_1,width=5)\n",
    "# help(tf.nn.max_pool)\n",
    "# --------Fully Connected--------\n",
    "def fully_connected(input_layer,num_outputs):\n",
    "    # First we find the needed shape of the multiplication weight matrix:\n",
    "    # The dimension will be (length of input) by (num_outputs)\n",
    "    # tf.pack modified to tf.stack\n",
    "    print(\"full_connected\")\n",
    "    print(input_layer.get_shape())#Tensor(\"Squeeze_25:0\", shape=(17,), dtype=float32)\n",
    "    print(tf.shape(input_layer))#Tensor(\"Shape_7:0\", shape=(1,), dtype=int32)\n",
    "    print(tf.stack([tf.shape(input_layer),[num_outputs]]))#Tensor(\"stack_9:0\", shape=(2, 1), dtype=int32)\n",
    "    weight_shape = tf.squeeze(tf.stack([tf.shape(input_layer),[num_outputs]]))\n",
    "    print(weight_shape)#Tensor(\"Squeeze_18:0\", shape=(2,), dtype=int32)\n",
    "    # Initialize such weight\n",
    "    weight = tf.random_normal(weight_shape,stddev=0.1)\n",
    "    print(weight_shape)#Tensor(\"Squeeze_18:0\", shape=(2,), dtype=int32)\n",
    "    # Initialize the bias\n",
    "    bias = tf.random_normal(shape=[num_outputs])\n",
    "    print(bias)#Tensor(\"random_normal_14:0\", shape=(5,), dtype=float32)\n",
    "    # Make the 1D input array into a 2D array for matrix multiplication\n",
    "    input_layer_2d = tf.expand_dims(input_layer,0)\n",
    "    # Perform the matrix multiplication and add the bias\n",
    "    full_output = tf.add(tf.matmul(input_layer_2d,weight),bias)\n",
    "    print(full_output)#Tensor(\"Add_4:0\", shape=(1, 5), dtype=float32)\n",
    "    # Get rid of extra dimensions\n",
    "    full_output_1d = tf.squeeze(full_output)\n",
    "    print(full_output_1d)#Tensor(\"Squeeze_23:0\", shape=(5,), dtype=float32)\n",
    "    return full_output_1d\n",
    "\n",
    "full_output_1 = fully_connected(maxpool_output_1,num_outputs=5)\n",
    "# Run graph\n",
    "# Initialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "feed_dict = {x_input_1d: data_1d}\n",
    "help(tf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(25,), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(1, 5, 1, 1) dtype=float32_ref> [[[[-0.26380596]]\n",
      "\n",
      "  [[-0.09360479]]\n",
      "\n",
      "  [[-1.03008616]]\n",
      "\n",
      "  [[-1.10657895]]\n",
      "\n",
      "  [[ 1.61247182]]]]\n",
      "Tensor(\"Squeeze:0\", shape=(21,), dtype=float32)\n",
      "Convolution Output---> [-5.25032902 -0.76354825  2.38797832 -2.96033144  1.4671458   2.20014429\n",
      " -4.3531661   1.08731711  1.35556352 -1.676211    0.55022949  0.87963533\n",
      " -1.12203455 -2.56535292  1.1218183  -0.49584508  1.55208647  4.10489845\n",
      "  1.92178488 -5.42443514  2.56427956]\n",
      "Tensor(\"Conv2D_1:0\", shape=(10, 3, 3, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Convolution Output\n",
    "print(x_input_1d)\n",
    "print(conv_filter_1,sess.run(conv_filter_1))\n",
    "print(conv_output_1)\n",
    "print(\"Convolution Output--->\",sess.run(conv_output_1,feed_dict=feed_dict))\n",
    "\n",
    "# test conv2d function\n",
    "input = tf.Variable(tf.random_normal([10,5,5,5])) # shape = [batch, in_height, in_width, in_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,7])) # shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID') #output shape = [batch, height, width, out_channels]\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          2.38797832  0.          1.4671458   2.20014429\n",
      "  0.          1.08731711  1.35556352  0.          0.55022949  0.87963533\n",
      "  0.          0.          1.1218183   0.          1.55208647  4.10489845\n",
      "  1.92178488  0.          2.56427956]\n",
      "[ 2.38797832  2.38797832  2.38797832  2.20014429  2.20014429  2.20014429\n",
      "  1.35556352  1.35556352  1.35556352  0.87963533  1.1218183   1.1218183\n",
      "  1.55208647  4.10489845  4.10489845  4.10489845  4.10489845]\n",
      "[ 3.42370319  0.64030182 -0.60373384 -0.64634222 -3.40978742]\n"
     ]
    }
   ],
   "source": [
    "# Activation Output\n",
    "print(sess.run(activation_output_1, feed_dict=feed_dict))\n",
    "\n",
    "# Max Pool Output\n",
    "print(sess.run(maxpool_output_1, feed_dict=feed_dict))\n",
    "\n",
    "# Fully Connected Output\n",
    "print(sess.run(full_output_1, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_1:0\", shape=(1, 10, 10, 1), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(2, 2, 1, 1) dtype=float32_ref>\n",
      "Tensor(\"Conv2D:0\", shape=(1, 5, 5, 1), dtype=float32)\n",
      "fully_2d\n",
      "Tensor(\"Squeeze_1:0\", shape=(4, 4), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(16,), dtype=float32)\n",
      "Tensor(\"Shape:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"Squeeze_2:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"random_normal_2:0\", shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------|\n",
    "# -------------------2D-data-------------------------|\n",
    "# ---------------------------------------------------|\n",
    "\n",
    "# Reset Graph\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Generate 2D data\n",
    "data_size = [10, 10]\n",
    "data_2d = np.random.normal(size=data_size)\n",
    "\n",
    "# --------Placeholder--------\n",
    "x_input_2d = tf.placeholder(dtype=tf.float32, shape=data_size)\n",
    "\n",
    "\n",
    "# Convolution\n",
    "def conv_layer_2d(input_2d, my_filter):\n",
    "    # Tensorflow's 'conv2d()' function only works with 4D arrays:\n",
    "    # [batch#, width, height, channels], we have 1 batch, and\n",
    "    # 1 channel, but we do have width AND height this time.\n",
    "    # So next we create the 4D array by inserting dimension 1's.\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # Note the stride difference below!\n",
    "    print(input_4d) #Tensor(\"ExpandDims_1:0\", shape=(1, 10, 10, 1), dtype=float32)\n",
    "    print(my_filter)#<tf.Variable 'Variable:0' shape=(2, 2, 1, 1) dtype=float32_ref>\n",
    "    convolution_output = tf.nn.conv2d(input_4d, filter=my_filter, strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    print(convolution_output) #Tensor(\"Conv2D:0\", shape=(1, 5, 5, 1), dtype=float32)\n",
    "    # Get rid of unnecessary dimensions\n",
    "    conv_output_2d = tf.squeeze(convolution_output)\n",
    "    return conv_output_2d\n",
    "\n",
    "\n",
    "# Create Convolutional Filter\n",
    "my_filter = tf.Variable(tf.random_normal(shape=[2, 2, 1, 1]))\n",
    "# Create Convolutional Layer\n",
    "my_convolution_output = conv_layer_2d(x_input_2d, my_filter)\n",
    "\n",
    "\n",
    "# --------Activation--------\n",
    "def activation(input_1d):\n",
    "    return tf.nn.relu(input_1d)\n",
    "\n",
    "\n",
    "# Create Activation Layer\n",
    "my_activation_output = activation(my_convolution_output)\n",
    "\n",
    "\n",
    "# --------Max Pool--------\n",
    "def max_pool(input_2d, width, height):\n",
    "    # Just like 'conv2d()' above, max_pool() works with 4D arrays.\n",
    "    # [batch_size=1, width=given, height=given, channels=1]\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # Perform the max pooling with strides = [1,1,1,1]\n",
    "    # If we wanted to increase the stride on our data dimension, say by\n",
    "    # a factor of '2', we put strides = [1, 2, 2, 1]\n",
    "    pool_output = tf.nn.max_pool(input_4d, ksize=[1, height, width, 1],\n",
    "                                 strides=[1, 1, 1, 1],\n",
    "                                 padding='VALID')\n",
    "    # Get rid of unnecessary dimensions\n",
    "    pool_output_2d = tf.squeeze(pool_output)\n",
    "    return pool_output_2d\n",
    "\n",
    "\n",
    "# Create Max-Pool Layer\n",
    "my_maxpool_output = max_pool(my_activation_output, width=2, height=2)\n",
    "\n",
    "\n",
    "# --------Fully Connected--------\n",
    "def fully_connected(input_layer, num_outputs):\n",
    "    print(\"fully_2d\")\n",
    "    # In order to connect our whole W byH 2d array, we first flatten it out to\n",
    "    # a W times H 1D array.\n",
    "    print(input_layer)#Tensor(\"Squeeze_1:0\", shape=(4, 4), dtype=float32)\n",
    "    flat_input = tf.reshape(input_layer, [-1])\n",
    "    print(flat_input)#Tensor(\"Reshape:0\", shape=(16,), dtype=float32)\n",
    "    # We then find out how long it is, and create an array for the shape of\n",
    "    # the multiplication weight = (WxH) by (num_outputs)\n",
    "    # tf.pack modified to tf.stack\n",
    "    print(tf.shape(flat_input))#Tensor(\"Shape:0\", shape=(1,), dtype=int32)\n",
    "    weight_shape = tf.squeeze(tf.stack([tf.shape(flat_input), [num_outputs]]))\n",
    "    print(weight_shape)#Tensor(\"Squeeze_2:0\", shape=(2,), dtype=int32)\n",
    "    # Initialize the weight\n",
    "    weight = tf.random_normal(weight_shape, stddev=0.1)\n",
    "    # Initialize the bias\n",
    "    bias = tf.random_normal(shape=[num_outputs])\n",
    "    print(bias)#Tensor(\"random_normal_2:0\", shape=(5,), dtype=float32)\n",
    "    # Now make the flat 1D array into a 2D array for multiplication\n",
    "    input_2d = tf.expand_dims(flat_input, 0)\n",
    "    # Multiply and add the bias\n",
    "    full_output = tf.add(tf.matmul(input_2d, weight), bias)\n",
    "    # Get rid of extra dimension\n",
    "    full_output_2d = tf.squeeze(full_output)\n",
    "    return full_output_2d\n",
    "\n",
    "# Create Fully Connected Layer\n",
    "my_full_output = fully_connected(my_maxpool_output, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "[[-0.5984751  -0.44325101  2.00933743 -1.11890626 -1.61312366]\n",
      " [-1.35967588 -0.46474075 -0.63701457 -0.72974437 -0.14482903]\n",
      " [-1.08951175 -0.78064382 -0.92946291 -0.9610787   1.92230773]\n",
      " [-1.56241274  0.97630847 -1.3101933  -0.33032238  0.22170272]\n",
      " [-0.62391078  0.42604107 -1.31330931  1.21313882  0.62701559]]\n",
      "[[ 0.          0.          2.00933743  0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          1.92230773]\n",
      " [ 0.          0.97630847  0.          0.          0.22170272]\n",
      " [ 0.          0.42604107  0.          1.21313882  0.62701559]]\n",
      "[[ 0.          2.00933743  2.00933743  0.        ]\n",
      " [ 0.          0.          0.          1.92230773]\n",
      " [ 0.97630847  0.97630847  0.          1.92230773]\n",
      " [ 0.97630847  0.97630847  1.21313882  1.21313882]]\n",
      "[ 0.53567219 -0.48650131 -0.62142676  0.13522327 -0.38255793]\n"
     ]
    }
   ],
   "source": [
    "# Run graph\n",
    "# Initialize Variables\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "feed_dict = {x_input_2d: data_2d}\n",
    "\n",
    "# Convolution Output\n",
    "print(sess.run(my_convolution_output, feed_dict=feed_dict))\n",
    "\n",
    "# Activation Output\n",
    "print(sess.run(my_activation_output, feed_dict=feed_dict))\n",
    "\n",
    "# Max Pool Output\n",
    "print(sess.run(my_maxpool_output, feed_dict=feed_dict))\n",
    "\n",
    "# Fully Connected Output\n",
    "print(sess.run(my_full_output, feed_dict=feed_dict))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
